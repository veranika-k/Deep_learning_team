{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00161f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\n",
      "ERROR: No matching distribution found for tensorflow\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef51c606",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m layers, Model\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mEfficientNetBackbone\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "class EfficientNetBackbone:\n",
    "    \"\"\"Pretrained EfficientNet backbone adapted for grayscale images\"\"\"\n",
    "    def __init__(self, model_name='EfficientNetB0', input_shape=(512, 512, 1), freeze=True):\n",
    "        self.model_name = model_name\n",
    "        self.input_shape = input_shape\n",
    "        self.freeze = freeze\n",
    "        self.model = self.build_backbone()\n",
    "        \n",
    "    def build_backbone(self):\n",
    "        \"\"\"Create backbone with channel adaptation for grayscale\"\"\"\n",
    "        # Input layer for grayscale images\n",
    "        inputs = layers.Input(shape=self.input_shape)\n",
    "        \n",
    "        # Convert 1-channel to 3-channel by replication\n",
    "        x = layers.Concatenate(axis=-1)([inputs, inputs, inputs])\n",
    "        \n",
    "        # Load pretrained EfficientNet\n",
    "        if self.model_name == 'EfficientNetB0':\n",
    "            base_model = tf.keras.applications.EfficientNetB0(\n",
    "                include_top=False, \n",
    "                weights='imagenet',\n",
    "                input_tensor=x\n",
    "            )\n",
    "            skip_layers = ['block2a_expand_activation', 'block3a_expand_activation']\n",
    "        elif self.model_name == 'EfficientNetB4':\n",
    "            base_model = tf.keras.applications.EfficientNetB4(\n",
    "                include_top=False, \n",
    "                weights='imagenet',\n",
    "                input_tensor=x\n",
    "            )\n",
    "            skip_layers = ['block2c_add', 'block4a_expand_activation']\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {self.model_name}\")\n",
    "        \n",
    "        # Freeze backbone if requested\n",
    "        if self.freeze:\n",
    "            base_model.trainable = False\n",
    "            \n",
    "        # Get skip connections\n",
    "        skip_outputs = [base_model.get_layer(name).output for name in skip_layers]\n",
    "        \n",
    "        return Model(inputs=inputs, outputs=[base_model.output] + skip_outputs, name=f\"{self.model_name}_Backbone\")\n",
    "\n",
    "class DeepLabV3Plus:\n",
    "    \"\"\"DeepLabV3+ decoder head for segmentation\"\"\"\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "    def build_decoder(self, inputs, skip_connections):\n",
    "        \"\"\"Build DeepLabV3+ decoder\"\"\"\n",
    "        # Unpack inputs\n",
    "        x = inputs[0]  # Main backbone output\n",
    "        skip1 = inputs[1]  # Low-level features (stride 4)\n",
    "        skip2 = inputs[2]  # Mid-level features (stride 8)\n",
    "        \n",
    "        # ASPP Module\n",
    "        x = self.aspp_module(x)\n",
    "        \n",
    "        # Decoder Path\n",
    "        x = self.decoder_path(x, skip1, skip2)\n",
    "        \n",
    "        # Output layer\n",
    "        return layers.Conv2D(self.num_classes, 1, activation='softmax', name='output')(x)\n",
    "    \n",
    "    def aspp_module(self, inputs):\n",
    "        \"\"\"Atrous Spatial Pyramid Pooling module\"\"\"\n",
    "        # Get input shape\n",
    "        input_shape = tf.shape(inputs)\n",
    "        h, w = input_shape[1], input_shape[2]\n",
    "        \n",
    "        # Branch 1: 1x1 Conv\n",
    "        b1 = layers.Conv2D(256, 1, padding='same', use_bias=False)(inputs)\n",
    "        b1 = layers.BatchNormalization()(b1)\n",
    "        b1 = layers.ReLU()(b1)\n",
    "        \n",
    "        # Branch 2: 3x3 Conv rate=6\n",
    "        b2 = layers.Conv2D(256, 3, padding='same', dilation_rate=6, use_bias=False)(inputs)\n",
    "        b2 = layers.BatchNormalization()(b2)\n",
    "        b2 = layers.ReLU()(b2)\n",
    "        \n",
    "        # Branch 3: 3x3 Conv rate=12\n",
    "        b3 = layers.Conv2D(256, 3, padding='same', dilation_rate=12, use_bias=False)(inputs)\n",
    "        b3 = layers.BatchNormalization()(b3)\n",
    "        b3 = layers.ReLU()(b3)\n",
    "        \n",
    "        # Branch 4: 3x3 Conv rate=18\n",
    "        b4 = layers.Conv2D(256, 3, padding='same', dilation_rate=18, use_bias=False)(inputs)\n",
    "        b4 = layers.BatchNormalization()(b4)\n",
    "        b4 = layers.ReLU()(b4)\n",
    "        \n",
    "        # Branch 5: Image Pooling\n",
    "        b5 = layers.GlobalAveragePooling2D()(inputs)\n",
    "        b5 = layers.Reshape((1, 1, -1))(b5)\n",
    "        b5 = layers.Conv2D(256, 1, use_bias=False)(b5)\n",
    "        b5 = layers.BatchNormalization()(b5)\n",
    "        b5 = layers.ReLU()(b5)\n",
    "        b5 = layers.UpSampling2D(size=(h, w), interpolation='bilinear')(b5)\n",
    "        \n",
    "        # Concatenate and project\n",
    "        x = layers.Concatenate()([b1, b2, b3, b4, b5])\n",
    "        x = layers.Conv2D(256, 1, padding='same', use_bias=False)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        return layers.ReLU(name='aspp_output')(x)\n",
    "    \n",
    "    def decoder_path(self, inputs, skip1, skip2):\n",
    "        \"\"\"Upsampling path with skip connections\"\"\"\n",
    "        # First upsample + skip connection\n",
    "        x = layers.UpSampling2D(size=(4, 4), interpolation='bilinear')(inputs)\n",
    "        \n",
    "        # Process low-level features\n",
    "        skip1 = layers.Conv2D(48, 1, padding='same', activation='relu')(skip1)\n",
    "        x = layers.Concatenate()([x, skip1])\n",
    "        \n",
    "        # Convolutional blocks\n",
    "        x = layers.Conv2D(256, 3, padding='same', activation='relu')(x)\n",
    "        x = layers.Conv2D(256, 3, padding='same', activation='relu')(x)\n",
    "        \n",
    "        # Second upsample + skip connection\n",
    "        x = layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(x)\n",
    "        skip2 = layers.Conv2D(32, 1, padding='same', activation='relu')(skip2)\n",
    "        x = layers.Concatenate()([x, skip2])\n",
    "        \n",
    "        # Final convolutions\n",
    "        x = layers.Conv2D(128, 3, padding='same', activation='relu')(x)\n",
    "        return layers.Conv2D(128, 3, padding='same', activation='relu', name='decoder_output')(x)\n",
    "\n",
    "class SegmentationModel:\n",
    "    \"\"\"Complete segmentation model with EfficientNet backbone and DeepLabV3+ head\"\"\"\n",
    "    def __init__(self, backbone_name='EfficientNetB0', num_classes=3, input_shape=(512, 512, 1)):\n",
    "        self.backbone_name = backbone_name\n",
    "        self.num_classes = num_classes\n",
    "        self.input_shape = input_shape\n",
    "        self.model = self.build_model()\n",
    "        \n",
    "    def build_model(self):\n",
    "        \"\"\"Assemble the complete model\"\"\"\n",
    "        # Build backbone\n",
    "        backbone = EfficientNetBackbone(\n",
    "            model_name=self.backbone_name,\n",
    "            input_shape=self.input_shape,\n",
    "            freeze=True\n",
    "        ).model\n",
    "        \n",
    "        # Get backbone outputs\n",
    "        backbone_outputs = backbone.outputs\n",
    "        backbone_input = backbone.input\n",
    "        \n",
    "        # Build decoder\n",
    "        decoder = DeepLabV3Plus(num_classes=self.num_classes)\n",
    "        outputs = decoder.build_decoder(backbone_outputs, backbone_outputs[1:])\n",
    "        \n",
    "        return Model(inputs=backbone_input, outputs=outputs, name='DeepLabV3Plus')\n",
    "    \n",
    "    def compile(self, learning_rate=1e-4):\n",
    "        \"\"\"Compile the model with standard settings\"\"\"\n",
    "        self.model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "    \n",
    "    def summary(self):\n",
    "        return self.model.summary()\n",
    "    \n",
    "    def unfreeze_backbone(self, unfreeze_layers=10):\n",
    "        \"\"\"Selectively unfreeze backbone layers for fine-tuning\"\"\"\n",
    "        # Unfreeze batch normalization layers first\n",
    "        for layer in self.model.layers:\n",
    "            if isinstance(layer, layers.BatchNormalization):\n",
    "                layer.trainable = True\n",
    "                \n",
    "        # Unfreeze last N layers of backbone\n",
    "        backbone = self.model.get_layer(f\"{self.backbone_name}_Backbone\")\n",
    "        for layer in backbone.layers[-unfreeze_layers:]:\n",
    "            if not isinstance(layer, layers.BatchNormalization):\n",
    "                layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdcca11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and compile model\n",
    "model = SegmentationModel(\n",
    "    backbone_name='EfficientNetB0',\n",
    "    num_classes=3,  # Background + your classes\n",
    "    input_shape=(512, 512, 1)\n",
    ")\n",
    "\n",
    "# See model architecture\n",
    "model.summary()\n",
    "\n",
    "# Compile with custom learning rate\n",
    "model.compile(learning_rate=1e-4)\n",
    "\n",
    "# Prepare dataset (example using tf.data)\n",
    "def load_data(image_path, mask_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_png(image, channels=1)\n",
    "    image = tf.image.resize(image, (512, 512))\n",
    "    \n",
    "    mask = tf.io.read_file(mask_path)\n",
    "    mask = tf.image.decode_png(mask, channels=1)\n",
    "    mask = tf.image.resize(mask, (512, 512), method='nearest')\n",
    "    \n",
    "    return image, mask\n",
    "\n",
    "# Create dataset\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_image_paths, train_mask_paths))\n",
    "train_ds = train_ds.map(load_data).batch(4)\n",
    "\n",
    "# Train initial model\n",
    "model.model.fit(train_ds, epochs=10)\n",
    "\n",
    "# Fine-tune with backbone unfrozen\n",
    "model.unfreeze_backbone(unfreeze_layers=15)\n",
    "model.compile(learning_rate=1e-5)\n",
    "model.model.fit(train_ds, epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
